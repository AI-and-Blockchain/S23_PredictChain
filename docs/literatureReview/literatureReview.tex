\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}

\title{Predict Chain}
\author{Matthew Pisano, William Hawkins}
\date{Spring 2023}

\begin{document}

    \maketitle

    \section{Introduction}

    Throughout our project, we have built upon, or have related to, the ideas of many other projects and papers.
    Some of these relations were explicit, such as our work with, and utilization of, various types of neural
    network.  Others were implicit, like the relation that our project has to the many other implementations of
    blockchain-based predictive projects.  It is important to acknowledge the influences that other works have
    has on our project, and it is equally important to analyze the similarities between our project and the
    other papers that we have referenced here.  Through this, we hope to better understand the perspective of
    others on similar topics to our project and hopefully improve upon our work and techniques in the future.


    \section{Related Work}
    % Please feel free to add any subsections as necessary to elaborate on different key facets of the related work.
    % However, your writing must follow a cohesive structure (i.e., it should not read like an annotated bibliography!).
    % But please include a table to compare specific attributes of the related work with your project.
    % Use the references.bib file to include the citations in biblatex format.

    \subsection{Long Short-Term Memory Networks}

    Long Short-Term Memory Networks\cite{LSTM} is one of the most influential, and useful, types of network that we work
    with in our project.  These networks introduce several important improvements on classical backpropagation-through-time
    recurrent neural networks.  One of the most notable contributions of LSTMs is their resolution of the 'exploding
    and vanishing gradients' problem.  In RNNs, it is common, and often disruptive, to observe error signals either
    grow to extreme values or disappear entirely when updating the weights of the model.  This either leads to
    extreme unpredictability in the next value that weights will take on, or it leads to the weights not being
    updated at all. In both cases, the effectiveness of the model is greatly impacted.  To help remedy this issue,
    the authors propose an architecture that enforces a 'constant \ldots error flow through internal states' inside the
    model.

    Through a series of experiments, that the authors propose in the paper, they show that LSTMs can outperform RNNs
    on long, minimal time lag data.  In this data, the window that the


    \subsection{Recurrent Sequence Modeling}

    One of the core aspects of our project is our usage of various types of neural network for predictive modeling.
    Most of our neural networks have some recurrent capability, specifically our RNN, LSTM, and GRU model types.
    Additionally, the data that we suggest that these models be trained on is sequential data, exactly what these
    types of model were designed for.  One paper that aligns especially closely with this component of our project
    is \textit{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}\cite{recurrentModeling}.

    Within this paper, the authors aim to evaluate the relative performance of RNNs, LSTM networks, and GRU networks.
    Similar to our use case, they perform this evaluation using sequential signal data, similar in nature to the
    stock market dataset that we use in our example.  Primarily, they focus on the LSTM and GRU variants of
    recurrent neural networks, rather than vanilla RNNs themselves.  As part of their evaluation, they compare
    the performance of LSTM unit, GRU unit, and tanh unit RNNs on modeling sequences of polyphonic music and
    speech signals.

    As part of their evaluation, they utilize three publicly available music datasets along with two internal speech
    datasets from Ubisoft.  The design of their experiment is to compare the negative log probabilities of the models
    after they are given 20 consecutive samples of audio data and are then tasked to predict the next 10 consecutive
    samples.  In the results that they gathered, they concluded that, overall, the GRU unit outperformed the other
    two evaluated models.  These results are amplified in the Ubisoft datasets, where GRU unis significantly
    outperform the others in one of the test cases and have similar performance to LSTMs in the other.  Additionally,
    in the Ubisoft Dataset B, the negative log-likelihood of the GRU model continues to shrink in a super-linear manner,
    whereas the other models become asymptotic.

    \subsubsection*{Relation to Our Work}

    This paper both confirmed some of the initial assumptions that we had about the models and changed some others.
    In the design of the project, we had initially marked GRU models to be better than all the other models.
    We denoted this perceived difference in output quality through the constant multiplier we applied to each of their
    $model\_complexity$ values.  After our analysis of this paper, we ran several tests to see if this paper's results
    also applied to our project.  In our tests, we noticed something very similar to the paper.  GRUs and LSTMs
    has more similar performance than we initially thought.  Meanwhile, vanilla RNNs lagged behind.  Based off of
    this evidence, we decided to update these values to better reflect the experimental performance of these models.

    \begin{table}[h!]
        \begin{center}
            \caption{$model\_complexity$ Values of Our Models}
            \label{tab:modelComplexity}
            \begin{tabular}{c|c|c}
                \textbf{Model} & \textbf{Initial Value} & \textbf{Final value}\\
                \hline
                MLP & 1 & 1\\
                RNN & 1.5 & 1.7\\
                LSTM & 2 & 2.2\\
                GRU & 2.5 & 2.4
            \end{tabular}
        \end{center}
    \end{table}

    In our changes, we amplified the values related to all RNN-based networks.  We also brought the complexities
    of the LSTM and GRU networks closer together.  This change was made to reflect their relative performances to
    each other and to more traditional RNNs.  Overall, this paper has been important for our project as it has
    helped us to reflect upon our models and how to best evaluate them.


    \pagebreak
    \bibliographystyle{abbrv}
    \bibliography{references}

\end{document}
